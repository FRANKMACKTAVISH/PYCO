!pip install pyspark

import pyspark
from pyspark.sql import SparkSession, SQLContext

practice = SparkSession.builder.appName('Rayuga').getOrCreate()
practice

MAPREDUCE

word_file_path = '/content/drive/MyDrive/DIU/Courses/Big-Data-and-IOT/lorem-class.txt'

word_count = practice.sparkContext.textFile(word_file_path).flatMap(lambda line: line.split())
word_count.collect()

word_count_mapping = word_count.map(lambda word: (word, 1))
word_count_mapping.collect()

word_count_sort = word_count_mapping.sortByKey()
word_count_sort.collect()

word_count_reduce = word_count_sort.reduceByKey(lambda a, b: a+b)
word_count_reduce.collect()

DATAFRAME WINDOW

time_province = practice.read.load('/content/drive/MyDrive/DIU/Courses/Big-Data-and-IOT/lab-5/TimeProvince.csv', sep=',', InferSchema=True, header=True, format='csv')
time_province.show()

from pyspark.sql import functions as Function

time_province_group = time_province.groupBy('province').agg(Function.collect_list('confirmed'), Function.count('confirmed'), Function.sum('confirmed'))
time_province_group.show()

from pyspark.sql.window import Window

time_province_window = Window.partitionBy(['province']).orderBy(Function.desc('confirmed'))
time_province.withColumn('rank', Function.rank().over(time_province_window)).show()


time_province.withColumn('lag', Function.lag('confirmed', 5).over(time_province_window)).show()

time_province_new_window = Window.partitionBy(['province']).orderBy(Function.desc('confirmed')).rowsBetween(-5, 0)
time_province.withColumn('rows between', Function.sum('confirmed').over(time_province_new_window)).show()






