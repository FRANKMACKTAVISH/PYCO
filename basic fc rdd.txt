basic of rdd

!pip install pyspark py4j

from google.colab import drive
drive.mount('/content/drive')

from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("create by any name").getOrCreate()

from google.colab import files
uploaded = files.upload()

how to create rdd from the text file

text_file_rdd = spark.sparkContext.textFile('yourfilename.txt')

text_file_rdd.take(10)

upper_rdd = text_file_rdd.map(lambda line: line.upper())

upper_rdd.take(10)

words_rdd = text_file_rdd.flatMap(lambda line: line.split())

words_rdd.take(10)


specific_word = "any-specific-word"

filtered_rdd = text_file_rdd.filter(lambda line: specific_word in line)

filtered_rdd.take(10)


count = text_file_rdd.count()

print("Number of values in the RDD:", count)

line_count = text_file_rdd.count()

print("Total number of lines:", line_count)

word_count = words_rdd.count()

print("Total number of words:", word_count)


first_element = your_rdd.first()
print("First Element:", first_element)

first_n_elements = your_rdd.take(5)
print("First 5 Elements:", first_n_elements)


max_value = your_rdd.max()
print("Max Value:", max_value)

min_value = your_rdd.min()
print("Min Value:", min_value)


final_rdd.saveAsTextFile("final_output.txt")

!cat final_output.txt/part-*

