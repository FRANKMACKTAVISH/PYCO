!pip install pyspark py4j
import pyspark
from pyspark.sql import SparkSession
hello = SparkSession.builder.appName("Taslima").getOrCreate()
hello
myList = [1,2,3,4,5,6,'a','b', "helloworld" ]
myRDD = spark.sparkContext.parallelize(myList,9)
myRDD.getNumPartitions()
myRDD.collect()
myRDD.count()
myRDD.take(4)
myRDD.take (1)
myRDD.first()

from google.colab import drive
drive.mount("/content/drive")

myfile=open(r"/content/drive/MyDrive/Colab Notebooks/a/123.txt")
myfile.read()
mytextRDD=hello.sparkContext.textFile("/content/drive/MyDrive/Colab Notebooks/a/lorem class.txt")
mytextRDD.getNumPartitions()
mytextRDD.collect()
mytextRDD.count()

from google.colab import drive
drive.mount("/content/drive")

mycsvRDD= spark.sparkContext.textFile("/content/drive/MyDrive/Colab Notebooks/a/FIFA-21 Complete.csv")
mycsvRDD.getNumPartitions()
mycsvRDD.collect()
mycsvRDD.count()

def Func(lines):
  lines=lines.split()
  return lines

splitRDD = mytextRDD.map(Func)


splitRDD.collect()
stopword = ['Lorem', 'Ipsum']
filterRDD = splitRDD.filter(lambda x: x not in stopword)
filterRDD.collect()



